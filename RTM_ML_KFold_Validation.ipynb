{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RTM ML KFold Validation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4acXdwZDIGvAlLjy2wI6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunocesarsa/GoogleColab_public/blob/master/RTM_ML_KFold_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igcEc3n5e6nX",
        "colab_type": "code",
        "outputId": "51392a15-ba1d-46e9-e385-31771fb7dd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "#Installing packages\n",
        "\n",
        "#Needed for step 1 - data generation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "!pip install pysptools\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prosail\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/52/d0c15ab469e8c82bc76a6b6cd614efbc60e43d09d5bacaa349170d229e91/prosail-2.0.5-py3-none-any.whl (149kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.17.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.47.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (42.0.2)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Installing collected packages: prosail\n",
            "Successfully installed prosail-2.0.5\n",
            "Collecting lhsmdu\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/f0/e714a4dae734bcd7228a09d74fff7dc5857dc3311cd72a3e07b09c85d088/lhsmdu-0.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Installing collected packages: lhsmdu\n",
            "Successfully installed lhsmdu-0.1\n",
            "Collecting pysptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/20/cef48129eff2bdcb282279138c09e6f04770a8fdcb3c1bb9a98fe4086d2d/pysptools-0.15.0.tar.gz (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 9.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pysptools\n",
            "  Building wheel for pysptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysptools: filename=pysptools-0.15.0-cp36-none-any.whl size=8133747 sha256=83571d48dd087ea5d7dbf4d114b33076e9488b89325219f1302443151bd73fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/60/be/a6719d91bfa59135201feb034c7069e4146aa576fc0dc9e624\n",
            "Successfully built pysptools\n",
            "Installing collected packages: pysptools\n",
            "Successfully installed pysptools-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEBQsBv2Tkdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#package to for operations on spectral data\n",
        "import pysptools as sptool \n",
        "from pysptools import distance\n",
        "#machine learning packages are imported later, nearer to the model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFXPCpQAT3W4",
        "colab_type": "text"
      },
      "source": [
        "First we create 3 datasets of 500, 1500 and 3000 samples\n",
        "\n",
        "Varying parameters: Cab, Cw, Cm and LAI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxYXMno0UHLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of samples\n",
        "train_n3000 = 3000\n",
        "train_n1500 = 1500\n",
        "train_n0500 = 500\n",
        "\n",
        "\n",
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "LHS_train3000 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n3000 ) #the package has a more advanced method but it is too slow to process\n",
        "LHS_train1500 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n1500 )\n",
        "LHS_train0500 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n0500 )\n",
        "\n",
        "#max_n=1 #this value should go from 1 to 2, so i make it change from 0 to 1 here and then add 1 later\n",
        "max_cab=79. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4acSDBZUpBF",
        "colab_type": "text"
      },
      "source": [
        "The next 2 snippets are functions needed for generating the data at sentinel resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i0Za6DEUxxl",
        "colab_type": "text"
      },
      "source": [
        "First a function for better control of the prosail call"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knuIs9XIUogn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in here I create a custom call for prosail, this allows me to more easily control the default values\n",
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.2\n",
        "  car=25.\n",
        "  cbrown=0.01\n",
        "  typelidf=1\n",
        "  lidfa = -0.35 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,hspot,tts,tto,psi,\n",
        "                                 typelidf, #lidfb=-0.15,\n",
        "                                 factor='SDR', rsoil=1., psoil=1.)\n",
        "  return(rho_out)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XcQwvOTVGX4",
        "colab_type": "text"
      },
      "source": [
        "Then a function to convert the input hyperspectral data to Sentinel 2A data using a weighted mean approach\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVTJySV8TCTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/S2_Response.csv\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy7VRBPIVUT2",
        "colab_type": "text"
      },
      "source": [
        "Now we create a function that generates the data given the n input samples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUpgRwkDVchl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function expects as input a PD dataframe with the columns properly named\n",
        "#notice if you change any defaults on the custom_prosail function then you have to go back and\n",
        "#change that\n",
        "#this function also transforms the hyperspectral data to sentinel data\n",
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrg-Zv8k1zA",
        "colab_type": "text"
      },
      "source": [
        "Now we can get the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3s-xITbgypQ",
        "colab_type": "code",
        "outputId": "afbbe466-f7f5-4ed7-8c93-489ae0b710e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "#preparing function inputs\n",
        "\n",
        "pd_traits0500 = pd.DataFrame.transpose(pd.DataFrame(LHS_train0500))\n",
        "pd_traits1500 = pd.DataFrame.transpose(pd.DataFrame(LHS_train1500))\n",
        "pd_traits3000 = pd.DataFrame.transpose(pd.DataFrame(LHS_train3000))\n",
        "\n",
        "pd_traits0500.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "pd_traits1500.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "pd_traits3000.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "#pd_traits0500[\"car\"]=pd_traits0500[\"car\"]*max_car+1 \n",
        "pd_traits0500[\"cab\"]=pd_traits0500[\"cab\"]*max_cab+1.\n",
        "pd_traits0500[\"cw\"] =pd_traits0500[\"cw\"] *max_cw+.001\n",
        "pd_traits0500[\"cm\"] =pd_traits0500[\"cm\"] *max_cm+.001\n",
        "pd_traits0500[\"lai\"]=pd_traits0500[\"lai\"]*max_lai+.25\n",
        "\n",
        "pd_traits1500[\"cab\"]=pd_traits1500[\"cab\"]*max_cab+1.\n",
        "pd_traits1500[\"cw\"] =pd_traits1500[\"cw\"] *max_cw+.001\n",
        "pd_traits1500[\"cm\"] =pd_traits1500[\"cm\"] *max_cm+.001\n",
        "pd_traits1500[\"lai\"]=pd_traits1500[\"lai\"]*max_lai+.25\n",
        "\n",
        "pd_traits3000[\"cab\"]=pd_traits3000[\"cab\"]*max_cab+1.\n",
        "pd_traits3000[\"cw\"] =pd_traits3000[\"cw\"] *max_cw+.001\n",
        "pd_traits3000[\"cm\"] =pd_traits3000[\"cm\"] *max_cm+.001\n",
        "pd_traits3000[\"lai\"]=pd_traits3000[\"lai\"]*max_lai+.25\n",
        "\n",
        "\n",
        "#pd_train_traits[\"n\"]=pd_t\n",
        "np_spectra0500 = Gen_spectra_data(pd_traits0500)\n",
        "np_spectra1500 = Gen_spectra_data(pd_traits1500)\n",
        "np_spectra3000 = Gen_spectra_data(pd_traits3000)\n",
        "\n",
        "print(np_spectra0500.shape)\n",
        "print(np_spectra1500.shape)\n",
        "print(np_spectra3000.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 13)\n",
            "(1500, 13)\n",
            "(3000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyGQcqlEND_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lets also create a numpy object for the tratis\n",
        "np_traits0500 = pd_traits0500.iloc[:,:].values\n",
        "np_traits1500 = pd_traits1500.iloc[:,:].values\n",
        "np_traits3000 = pd_traits3000.iloc[:,:].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWSkV4lnmju4",
        "colab_type": "text"
      },
      "source": [
        "Now we need to K-fold the data so we can do the LOOCV - Leave one out cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbGyFMZpkyKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold # import KFol\n",
        "\n",
        "#this command is enough to set u the k-fold\n",
        "kf = KFold(n_splits=5) # Define the split \n",
        "\n",
        "#test spot\n",
        "#X = np_spectra0500\n",
        "#Y = np.arange(len(np_spectra0500)) #this is simply a place holder\n",
        "\n",
        "#The kfold of sklearn doesn't actually randomize the folding but that is ok because\n",
        "#the samples were generated randomly anyway. \n",
        "#k = 1\n",
        "#for train_index, test_index in kf.split(X):\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  #X_train, X_test = X[train_index], X[test_index]\n",
        "  #y_train, y_test = Y[train_index], Y[test_index]\n",
        "  #print(k)\n",
        "  #print(train_index.shape)\n",
        "  #print(test_index.shape)\n",
        "  #k=k+1\n",
        "#uncomment above to see how it works\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKHSo6-x-zlm",
        "colab_type": "text"
      },
      "source": [
        "Now we set up all the machine learning models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCfuYM9o-7aL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "977888c7-5c14-4e1d-e724-6000b4bffc33"
      },
      "source": [
        "#machine learning stuff\n",
        "\n",
        "#NEURAL NETWORK - Keras will be updated soon so this colab will also have to be changed\n",
        "from sklearn.neural_network import MLPRegressor as ANN_reg #this is a simpler neural network package\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#ignore the warning for now\n",
        "\n",
        "#Random FOREST\n",
        "# Fitting Random Forest Regression to the dataset \n",
        "# import the regressor \n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#Gaussian processes\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
        "\n",
        "#initializing the ANN\n",
        "ann_ml = Sequential()\n",
        "#ann_ml.add(Dense(9, input_dim=9, activation='linear'))\n",
        "ann_ml.add(Dense(10, input_dim=9, activation='tanh'))\n",
        "ann_ml.add(Dense(6, activation='relu'))\n",
        "ann_ml.add(Dense(4)) #indeed this ha to be added in this case without any activ function, the R script added this on its own\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the keras model\n",
        "ann_ml.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#initializing the random forest\n",
        "rfr_ml = RandomForestRegressor(n_estimators=1000,random_state=0,\n",
        "                              min_samples_leaf=5,min_samples_split=5,verbose=1)\n",
        "#initializing the gaussian process\n",
        "gpr_ml = GaussianProcessRegressor(n_restarts_optimizer=50,\n",
        "                                        normalize_y=True,\n",
        "                                        random_state=0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poiuOfc5OFdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(np_traits0500)\n",
        "#print(pd_traits0500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCSG1JL-VC9F",
        "colab_type": "text"
      },
      "source": [
        "Creating an empty pandas dataframe to store the output of the models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu43ZWxpVJnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names=[\"Model\",\n",
        "              \"NSamples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeiXJox5vE-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a df to receive the data\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#we pick only the bands at 20m resolution - i reckon it is actually peaceful to use everything.. \n",
        "\n",
        "#first we subset the bands to the 20m resolution only\n",
        "#S2A_SR_AV_B1\tS2A_SR_AV_B2\tS2A_SR_AV_B3\t\n",
        "#S2A_SR_AV_B4\tS2A_SR_AV_B5\tS2A_SR_AV_B6\t\n",
        "#S2A_SR_AV_B7\tS2A_SR_AV_B8\tS2A_SR_AV_B8A\t\n",
        "#S2A_SR_AV_B9\tS2A_SR_AV_B10\tS2A_SR_AV_B11\t\n",
        "#S2A_SR_AV_B12\n",
        "\n",
        "train_df_0500 = np_spectra0500[:,[1,2,3,4,5,6,8,11,12]]\n",
        "train_df_1500 = np_spectra1500[:,[1,2,3,4,5,6,8,11,12]]\n",
        "train_df_3000 = np_spectra3000[:,[1,2,3,4,5,6,8,11,12]]\n",
        "\n",
        "\n",
        "#importing metric functions\n",
        "from sklearn import metrics\n",
        "\n",
        "#the ANN requires that we transform the variables\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "scaler_0500 = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSWynVJhQ-Im",
        "colab_type": "text"
      },
      "source": [
        "Measuring the accuraccy the of the regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPHq4ctgAEqN",
        "colab_type": "code",
        "outputId": "68b02d9a-9ad8-4b8b-aa7a-549bb2d6c414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "#with 500 samples\n",
        "k=1\n",
        "for train_index, test_index in kf.split(train_df_0500):\n",
        "\n",
        "  #subsetting for ith k-fold\n",
        "  X_train, X_test = train_df_0500[train_index], train_df_0500[test_index]\n",
        "  Y_train, Y_test = np_traits0500[train_index], np_traits0500[test_index]\n",
        "  label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  #ANN - Training \n",
        "  scaler_0500.fit(Y_train)\n",
        "  Y_train_norm = scaler_0500.transform(Y_train)\n",
        "  ann_ml.fit(X_train,Y_train_norm,epochs=1500,verbose=0)\n",
        "\n",
        "  #RF - Training n\n",
        "  rfr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #GPR - Training \n",
        "  gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #Prediction\n",
        "  y_ann_0500 = scaler_0500.inverse_transform(ann_ml.predict(X_test))\n",
        "  y_rfr_0500 = rfr_ml.predict(X_test)\n",
        "  y_gpr_0500 = gpr_ml.predict(X_test)\n",
        "\n",
        "\n",
        "  for i in range(n_traits):\n",
        "\n",
        "    #creating the temp list\n",
        "    ann_temp_list = {\"Model\":\"ANN\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_ann_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_ann_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_ann_0500[:,i])}\n",
        "\n",
        "    rfr_temp_list = {\"Model\":\"RFr\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_rfr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_rfr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_rfr_0500[:,i])}\n",
        "\n",
        "    gpr_temp_list = {\"Model\":\"GPR\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "\n",
        "    #appending to the dataframe\n",
        "    df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "  k = k+1\n",
        "\n",
        "df_metrics.to_csv(\"run0500.csv\",sep=\";\",decimal=\",\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0lod2wBvKpZ",
        "colab_type": "code",
        "outputId": "c955fa37-90a2-411b-cbb2-461deb83c13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#the ANN requires that we transform the variables\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "scaler_1500 = MinMaxScaler()\n",
        "\n",
        "#with 500 samples\n",
        "k=1\n",
        "for train_index, test_index in kf.split(train_df_1500):\n",
        "\n",
        "  #subsetting for ith k-fold\n",
        "  X_train, X_test = train_df_1500[train_index], train_df_1500[test_index]\n",
        "  Y_train, Y_test = np_traits1500[train_index], np_traits1500[test_index]\n",
        "  label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  #ANN - Training \n",
        "  scaler_1500.fit(Y_train)\n",
        "  Y_train_norm = scaler_1500.transform(Y_train)\n",
        "  ann_ml.fit(X_train,Y_train_norm,epochs=1500,verbose=0)\n",
        "\n",
        "  #RF - Training n\n",
        "  rfr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #GPR - Training \n",
        "  gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #Prediction\n",
        "  y_ann_1500 = scaler_0500.inverse_transform(ann_ml.predict(X_test))\n",
        "  y_rfr_1500 = rfr_ml.predict(X_test)\n",
        "  y_gpr_1500 = gpr_ml.predict(X_test)\n",
        "\n",
        "\n",
        "  for i in range(n_traits):\n",
        "\n",
        "    #creating the temp list\n",
        "    ann_temp_list = {\"Model\":\"ANN\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_ann_1500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_ann_1500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_ann_1500[:,i])}\n",
        "\n",
        "    rfr_temp_list = {\"Model\":\"RFr\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_rfr_1500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_rfr_1500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_rfr_1500[:,i])}\n",
        "\n",
        "    gpr_temp_list = {\"Model\":\"GPR\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_1500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_1500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_gpr_1500[:,i])}\n",
        "\n",
        "  #appending to the dataframe\n",
        "    df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "  k = k+1\n",
        "\n",
        "df_metrics.to_csv(\"run1500.csv\",sep=\";\",decimal=\",\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oIuz7N50Zy7",
        "colab_type": "code",
        "outputId": "fa60934e-0310-45ce-b181-56e4bda764e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#the ANN requires that we transform the variables\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "scaler_3000 = MinMaxScaler()\n",
        "\n",
        "#with 500 samples\n",
        "k=1\n",
        "for train_index, test_index in kf.split(train_df_3000):\n",
        "\n",
        "  #subsetting for ith k-fold\n",
        "  X_train, X_test = train_df_3000[train_index], train_df_3000[test_index]\n",
        "  Y_train, Y_test = np_traits3000[train_index], np_traits3000[test_index]\n",
        "  label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  #ANN - Training \n",
        "  scaler_3000.fit(Y_train)\n",
        "  Y_train_norm = scaler_3000.transform(Y_train)\n",
        "  ann_ml.fit(X_train,Y_train_norm,epochs=3000,verbose=0)\n",
        "\n",
        "  #RF - Training n\n",
        "  rfr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #GPR - Training \n",
        "  gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #Prediction\n",
        "  y_ann_3000 = scaler_0500.inverse_transform(ann_ml.predict(X_test))\n",
        "  y_rfr_3000 = rfr_ml.predict(X_test)\n",
        "  y_gpr_3000 = gpr_ml.predict(X_test)\n",
        "\n",
        "\n",
        "  for i in range(n_traits):\n",
        "\n",
        "    #creating the temp list\n",
        "    ann_temp_list = {\"Model\":\"ANN\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_ann_3000[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_ann_3000[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_ann_3000[:,i])}\n",
        "\n",
        "    rfr_temp_list = {\"Model\":\"RFr\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_rfr_3000[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_rfr_3000[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_rfr_3000[:,i])}\n",
        "\n",
        "    gpr_temp_list = {\"Model\":\"GPR\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_3000[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_3000[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_gpr_3000[:,i])}\n",
        "\n",
        "  #appending to the dataframe\n",
        "    df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "  k = k+1\n",
        "\n",
        "df_metrics.to_csv(\"run3000.csv\",sep=\";\",decimal=\",\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCzendkh3Kel",
        "colab_type": "text"
      },
      "source": [
        "Finished for now, data is saved, better to make the plots in R "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRPPMaNr3J4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e8be342e-01c7-4309-d7e2-678bf47c044f"
      },
      "source": [
        "df_metrics"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>NSamples</th>\n",
              "      <th>Variable</th>\n",
              "      <th>Fold_nr</th>\n",
              "      <th>ExplVar</th>\n",
              "      <th>Max_err</th>\n",
              "      <th>Mean_abs_Err</th>\n",
              "      <th>Mean_sqr_err</th>\n",
              "      <th>Median_abs_err</th>\n",
              "      <th>r2</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ANN</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.866581</td>\n",
              "      <td>34.295177</td>\n",
              "      <td>6.610323</td>\n",
              "      <td>8.103551e+01</td>\n",
              "      <td>5.192078</td>\n",
              "      <td>0.859116</td>\n",
              "      <td>62.270372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RFr</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.980351</td>\n",
              "      <td>19.078419</td>\n",
              "      <td>1.696964</td>\n",
              "      <td>1.153273e+01</td>\n",
              "      <td>0.869261</td>\n",
              "      <td>0.979950</td>\n",
              "      <td>13.059086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GPR</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.997047</td>\n",
              "      <td>12.743022</td>\n",
              "      <td>0.248184</td>\n",
              "      <td>1.724153e+00</td>\n",
              "      <td>0.028984</td>\n",
              "      <td>0.997002</td>\n",
              "      <td>7.798714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ANN</td>\n",
              "      <td>500</td>\n",
              "      <td>cw</td>\n",
              "      <td>1</td>\n",
              "      <td>0.646119</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>2.456110e-06</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.641561</td>\n",
              "      <td>39.529106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RFr</td>\n",
              "      <td>500</td>\n",
              "      <td>cw</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.015038</td>\n",
              "      <td>0.004745</td>\n",
              "      <td>0.002378</td>\n",
              "      <td>6.955564e-06</td>\n",
              "      <td>0.002479</td>\n",
              "      <td>-0.015080</td>\n",
              "      <td>73.727249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>RFr</td>\n",
              "      <td>3000</td>\n",
              "      <td>cm</td>\n",
              "      <td>5</td>\n",
              "      <td>0.275127</td>\n",
              "      <td>0.004242</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>3.912771e-06</td>\n",
              "      <td>0.001668</td>\n",
              "      <td>0.274110</td>\n",
              "      <td>53.875201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>GPR</td>\n",
              "      <td>3000</td>\n",
              "      <td>cm</td>\n",
              "      <td>5</td>\n",
              "      <td>0.999987</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>7.038466e-11</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.999987</td>\n",
              "      <td>0.124999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>ANN</td>\n",
              "      <td>3000</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.126453</td>\n",
              "      <td>6.508900</td>\n",
              "      <td>2.233380</td>\n",
              "      <td>6.909471e+00</td>\n",
              "      <td>2.163895</td>\n",
              "      <td>0.126442</td>\n",
              "      <td>103.211648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>RFr</td>\n",
              "      <td>3000</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.868487</td>\n",
              "      <td>5.878916</td>\n",
              "      <td>0.615887</td>\n",
              "      <td>1.044128e+00</td>\n",
              "      <td>0.333719</td>\n",
              "      <td>0.867992</td>\n",
              "      <td>18.806096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>GPR</td>\n",
              "      <td>3000</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.999965</td>\n",
              "      <td>0.165722</td>\n",
              "      <td>0.009622</td>\n",
              "      <td>2.780015e-04</td>\n",
              "      <td>0.006423</td>\n",
              "      <td>0.999965</td>\n",
              "      <td>0.387152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Model NSamples Variable  ... Median_abs_err        r2        MAPE\n",
              "0     ANN      500      cab  ...       5.192078  0.859116   62.270372\n",
              "1     RFr      500      cab  ...       0.869261  0.979950   13.059086\n",
              "2     GPR      500      cab  ...       0.028984  0.997002    7.798714\n",
              "3     ANN      500       cw  ...       0.001050  0.641561   39.529106\n",
              "4     RFr      500       cw  ...       0.002479 -0.015080   73.727249\n",
              "..    ...      ...      ...  ...            ...       ...         ...\n",
              "175   RFr     3000       cm  ...       0.001668  0.274110   53.875201\n",
              "176   GPR     3000       cm  ...       0.000002  0.999987    0.124999\n",
              "177   ANN     3000      lai  ...       2.163895  0.126442  103.211648\n",
              "178   RFr     3000      lai  ...       0.333719  0.867992   18.806096\n",
              "179   GPR     3000      lai  ...       0.006423  0.999965    0.387152\n",
              "\n",
              "[180 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}