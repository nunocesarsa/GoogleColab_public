{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSRP Lunch - KFolding ML Simulated inversion V02.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMn70Ks2Dg1EOcQD3Ktmctm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunocesarsa/GoogleColab_public/blob/master/DSRP_Lunch_KFolding_ML_Simulated_inversion_V02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aYzvp2eS-am",
        "colab_type": "code",
        "outputId": "10a1847d-3c1f-4f01-c52f-62d5d67f0e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#loading up my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Perhaps this step can be skipped by saving directly to the workspace\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAw7ZP-dRZVS",
        "colab_type": "text"
      },
      "source": [
        "Loading up all the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfQQg-gsPHFO",
        "colab_type": "code",
        "outputId": "85ee2159-39ec-461e-da6e-76550d4ec587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#package instalation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "!pip install pysptools\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prosail in /usr/local/lib/python3.6/dist-packages (2.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.47.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (45.1.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.2.0)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Requirement already satisfied: lhsmdu in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Requirement already satisfied: pysptools in /usr/local/lib/python3.6/dist-packages (0.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKBHNWSSXLyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "#the beutiful R like data frame\n",
        "import pandas as pd\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#a few more stuff for random\n",
        "import random as rdm\n",
        "import math\n",
        "\n",
        "\n",
        "#package to for operations on spectral data\n",
        "import pysptools as sptool \n",
        "from pysptools import distance\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "#machine learning stuff\n",
        "#NEURAL NETWORK - Keras will be updated soon so this colab will also have to be changed\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor as ANN_reg #this is a simpler neural network package\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#ignore the warning for now\n",
        "\n",
        "#Random FOREST\n",
        "# Fitting Random Forest Regression to the dataset \n",
        "# import the regressor \n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#Gaussian processes\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
        "\n",
        "#aux functions\n",
        "from sklearn.preprocessing import MinMaxScaler #this is to standardize the input data [not used for now]\n",
        "from sklearn import metrics\n",
        "\n",
        "#for model storing -sklearn\n",
        "import pickle\n",
        "\n",
        "#for model storing keras\n",
        "from keras.models import model_from_json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2IaL6Wv6uso",
        "colab_type": "text"
      },
      "source": [
        "# Custom functions to facilitate the calling of the prosail and convolution of the spectral wavelength to sentinel data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tArZkvpYYw17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  #tts=sol_zen #solar zenith angle\n",
        "  #tto=inc_zen #sensor zenith angle\n",
        "  #psi=raa\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9wEojgY-VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2FIDZFMZEAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJVeKoovgNlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to store the model outputs onto a dictionary\n",
        "\n",
        "column_names=[\"Model\",\n",
        "              \"NSamples\",\n",
        "              \"OutOfBag\",\n",
        "              \"KFold_tr_samples\",\n",
        "              \"KFold_vl_samples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\",\n",
        "              \"ModelName\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "\n",
        "\n",
        "def calc_metrics(MDL,Samples,oob_samples,kf_tr,kf_vl,Variable,Fold,Ref,Pred,Modelname):\n",
        "\n",
        "  out_list = {\"Model\":MDL,\n",
        "              \"NSamples\":Samples,\n",
        "              \"OutOfBag\":oob_samples,\n",
        "              \"KFold_tr_samples\":kf_tr,\n",
        "              \"KFold_vl_samples\":kf_vl,\n",
        "              \"Variable\":Variable,\n",
        "              \"Fold_nr\":Fold,\n",
        "              \"ExplVar\": metrics.explained_variance_score(Ref, Pred),\n",
        "              \"Max_err\": metrics.max_error(Ref, Pred),\n",
        "              \"Mean_abs_Err\": metrics.mean_absolute_error(Ref, Pred),\n",
        "              \"Mean_sqr_err\": metrics.mean_squared_error(Ref, Pred),\n",
        "              \"Median_abs_err\" : metrics.median_absolute_error(Ref, Pred),\n",
        "              \"r2\": metrics.r2_score(Ref, Pred),\n",
        "              \"MAPE\": mean_absolute_percentage_error(Ref, Pred),\n",
        "              \"ModelName\":Modelname}\n",
        "\n",
        "\n",
        "  return out_list\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9JlpBhwXU-Q",
        "colab_type": "text"
      },
      "source": [
        "#Important section Generating a trait space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9TXXAtnXPfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of samples\n",
        "#train_n3000 = 3000\n",
        "#train_n1500 = 1500\n",
        "#train_n0500 = 500\n",
        "\n",
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "#LHS_train3000 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n3000 ) #the package has a more advanced method but it is too slow to process\n",
        "#LHS_train1500 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n1500 )\n",
        "#LHS_train0500 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n0500 )\n",
        "\n",
        "#max_n=1 #this value should go from 1 to 2, so i make it change from 0 to 1 here and then add 1 later\n",
        "max_cab=79. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.005\n",
        "min_cm = 0.005\n",
        "min_lai = .5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41AJx1WhIFmi",
        "colab_type": "text"
      },
      "source": [
        "Setting up the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0USEUladhTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Random forest\n",
        "#initializing the random forest\n",
        "rfr_ml = RandomForestRegressor(n_estimators=1000,random_state=0,\n",
        "                               min_samples_leaf=5,\n",
        "                               min_samples_split=5,\n",
        "                               verbose=0)\n",
        "\n",
        "#single task Artificial neural network\n",
        "ann_ml = Sequential()\n",
        "#ann_ml.add(Dense(9, input_dim=9, activation='linear'))\n",
        "ann_ml.add(Dense(10, input_dim=9, activation='tanh'))\n",
        "ann_ml.add(Dense(6, activation='relu'))\n",
        "ann_ml.add(Dense(4)) #indeed this ha to be added in this case without any activ function, the R script added this on its own\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the keras model\n",
        "ann_ml.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#gaussian processes with default acquisition function\n",
        "#initializing the gaussian process\n",
        "gpr_ml = GaussianProcessRegressor(n_restarts_optimizer=50,\n",
        "                                  normalize_y=True,\n",
        "                                  random_state=0)\n",
        "\n",
        "#multi-task neural network - 32 by 16 by 8 per task\n",
        "inputs = Input(shape=(9,))\n",
        "sub1 = Dense(32, activation='tanh')(inputs)\n",
        "sub2 = Dense(16, activation='tanh')(sub1)\n",
        "cab1 = Dense(8, activation='sigmoid')(sub2)\n",
        "cw1  = Dense(8, activation='sigmoid')(sub2)\n",
        "cm1  = Dense(8, activation='sigmoid')(sub2)\n",
        "lai1 = Dense(8, activation='sigmoid')(sub2)\n",
        "cab2 = Dense(1, activation='linear')(cab1)\n",
        "cw2  = Dense(1, activation='linear')(cw1)\n",
        "cm2  = Dense(1, activation='linear')(cm1)\n",
        "lai2 = Dense(1, activation='linear')(lai1)\n",
        "\n",
        "\n",
        "#building the model using keras api\n",
        "model = Model(inputs=inputs, outputs=[cab2,cw2,cm2,lai2])\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FT8DsIG5Qw_",
        "colab_type": "text"
      },
      "source": [
        "# Main function Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJ7JH6p2Yox",
        "colab_type": "text"
      },
      "source": [
        "This is a confusing loop at first sight.\n",
        "\n",
        "First it generates a number of samples (j) based on input, then k-folds the data and runs all models storing their accuraccy results. Then stores all data into a list so i can operate on it elsewhere. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGWTtaoniL9i",
        "colab_type": "code",
        "outputId": "f2979015-0637-4216-fcd1-6e26f6980306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#setting up the limits of the run\n",
        "\n",
        "#minimium samples\n",
        "n_samples = 200\n",
        "n_points = np.arange(n_samples,3000+1,200)\n",
        "print(\"Loop for the following set of LHS samples:\",n_points)\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop for the following set of LHS samples: [ 200  400  600  800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800\n",
            " 3000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKRDd1AH2KjK",
        "colab_type": "code",
        "outputId": "8122d6df-298c-4bb1-a31b-b5e230ca2f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "### storing the dataset\n",
        "\n",
        "#this is for the training dataset\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "#this is for the out of bag\n",
        "df_metrics_valid = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#a path to store the models \n",
        "path2folder = \"/content/drive/My Drive/DSRP_Lunch_outputs/Models/\"\n",
        "\n",
        "#minimium samples\n",
        "#n_samples = 500\n",
        "#n_points = np.arange(n_samples,6000+1,500)\n",
        "\n",
        "print(\"Loop for the following set of LHS samples:\",n_points)\n",
        "\n",
        "#this command is enough to set u the k-fold\n",
        "fold_nr=5\n",
        "kf = KFold(n_splits=fold_nr,shuffle=True,random_state=0) # Define the split \n",
        "\n",
        "#this next comment snippet shouldn't be needed anymore\n",
        "\n",
        "##here we generate the out of bag test dataset of 1000 points that is used as out-of-bag testing\n",
        "#LHS_valid = lhsmdu.createRandomStandardUniformMatrix(n_traits,1000)\n",
        "#valid_pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_valid))\n",
        "#valid_pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "#valid_pd_trait[\"cab\"]=valid_pd_trait[\"cab\"]*max_cab+min_cab\n",
        "#valid_pd_trait[\"cw\"] =valid_pd_trait[\"cw\"] *max_cw+min_cw\n",
        "#valid_pd_trait[\"cm\"] =valid_pd_trait[\"cm\"] *max_cm+min_cw\n",
        "#valid_pd_trait[\"lai\"]=valid_pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "#valid_np_trait = valid_pd_trait.iloc[:,:].values\n",
        "#valid_np_spect = Gen_spectra_data(valid_pd_trait)\n",
        "#valid_df_spect = valid_np_spect[:,[1,2,3,4,5,6,8,11,12]]\n",
        "\n",
        "for j in n_points:\n",
        "  print(\"Generating trait table for\",j,\"samples\")\n",
        "\n",
        "  #this section generates trait space - careful with the column names as this is significant later\n",
        "  LHS_train = lhsmdu.createRandomStandardUniformMatrix(n_traits,j)\n",
        "  pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_train))\n",
        "  pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  pd_trait[\"cab\"]=pd_trait[\"cab\"]*max_cab+min_cab\n",
        "  pd_trait[\"cw\"] =pd_trait[\"cw\"] *max_cw+min_cw\n",
        "  pd_trait[\"cm\"] =pd_trait[\"cm\"] *max_cm+min_cw\n",
        "  pd_trait[\"lai\"]=pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "  #lets also create a numpy object for the traits - this is for ease of operation later\n",
        "  np_trait = pd_trait.iloc[:,:].values\n",
        "\n",
        "  #until here its instantaneous\n",
        "\n",
        "  #now we first generate the date in hyperspectral, convolute it to S2 while iterating through the entire given trait space\n",
        "  np_spect = Gen_spectra_data(pd_trait)[:,[1,2,3,4,5,6,8,11,12]]\n",
        "  #trait_df = np_spectra\n",
        "\n",
        "  #now from these generated samples, we take out 10% of the points (aproximately, sometimes its a decimal more, depends on the number)\n",
        "  #creating row identifiers\n",
        "  #this gets the length for the entire list\n",
        "  index = list(range(len(np_spect)))\n",
        "\n",
        "  index10 = rdm.sample(index,math.ceil(len(index)*.1)) #randomly selects 10% of te data (aproximately)\n",
        "  index90 = [x for x in index if x not in index10] #makes a list set wit the reamining\n",
        "  #once we have the list of point, we can subset\n",
        "\n",
        "  vl_trait_df = np_trait[index10,]\n",
        "  tr_trait_df = np_trait[index90,]\n",
        "  \n",
        "  vl_spect_df = np_spect[index10,]\n",
        "  tr_spect_df = np_spect[index90,]\n",
        "\n",
        "  #this is a function to re-scale inputs for the neural networks\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  print(\"Of which, 10% were taken as out-of-bag:\", len(vl_trait_df))\n",
        "\n",
        "  #now we have to iterate per fold, train and store the prediction\n",
        "  k=1\n",
        "  for train_index, test_index in kf.split(tr_spect_df):\n",
        "\n",
        "    #print(\"Calculating fold \",k,\"of\",fold_nr)\n",
        "    X_train, X_test = tr_spect_df[train_index], tr_spect_df[test_index]\n",
        "    Y_train, Y_test = tr_trait_df[train_index], tr_trait_df[test_index]\n",
        "    label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "    #training the machine learning models\n",
        "    #random forest\n",
        "    rfr_ml.fit(X_train,Y_train)\n",
        "    #shallow multi objective single task neural network\n",
        "    scaler.fit(Y_train)\n",
        "    Y_train_norm = scaler.transform(Y_train)\n",
        "    ann_ml.fit(X_train,Y_train_norm,epochs=500,verbose=0)\n",
        "\n",
        "    #GPR - Training \n",
        "    gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "    #\"Shallow\" Multi task ANN training\n",
        "    cab_train = Y_train_norm[:,0]\n",
        "    cw_train = Y_train_norm[:,1]\n",
        "    cm_train = Y_train_norm[:,2]\n",
        "    lai_train = Y_train_norm[:,3]\n",
        "    model.fit(X_train,[cab_train,cw_train,cm_train,lai_train],epochs=250,verbose=0)#hides spam on the output\n",
        "\n",
        "\n",
        "    #Prediction on training fold\n",
        "    y_ann = scaler.inverse_transform(ann_ml.predict(X_test))\n",
        "    y_rfr = rfr_ml.predict(X_test)\n",
        "    y_gpr = gpr_ml.predict(X_test)\n",
        "\n",
        "    #the multi task is a bit more complicated\n",
        "    Y_mtn = model.predict(X_test)\n",
        "    cab_pred = Y_mtn[0]\n",
        "    cw_pred  = Y_mtn[1]\n",
        "    cm_pred  = Y_mtn[2]\n",
        "    lai_pred = Y_mtn[3]\n",
        "    np_Y_ann_pred = np.array(cab_pred)\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,cw_pred))\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,cm_pred))\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,lai_pred))\n",
        "\n",
        "    y_mtn = scaler.inverse_transform(np_Y_ann_pred)\n",
        "\n",
        "    #prediction on out of bag\n",
        "    y_ann_valid = scaler.inverse_transform(ann_ml.predict(vl_spect_df))\n",
        "    y_rfr_valid = rfr_ml.predict(vl_spect_df)\n",
        "    y_gpr_valid = gpr_ml.predict(vl_spect_df)\n",
        "\n",
        "    Y_mtn_valid = model.predict(vl_spect_df)\n",
        "    cab_pred_valid = Y_mtn_valid[0]\n",
        "    cw_pred_valid  = Y_mtn_valid[1]\n",
        "    cm_pred_valid  = Y_mtn_valid[2]\n",
        "    lai_pred_valid = Y_mtn_valid[3]\n",
        "    np_Y_ann_pred_valid = np.array(cab_pred_valid)\n",
        "    np_Y_ann_pred_valid = np.hstack((np_Y_ann_pred_valid,cw_pred_valid))\n",
        "    np_Y_ann_pred_valid = np.hstack((np_Y_ann_pred_valid,cm_pred_valid))\n",
        "    np_Y_ann_pred_valid = np.hstack((np_Y_ann_pred_valid,lai_pred_valid))\n",
        "\n",
        "    y_mtn_valid = scaler.inverse_transform(np_Y_ann_pred_valid)\n",
        "\n",
        "\n",
        "    #setting the model filenames\n",
        "    #zfill is a handy function to standardize the output, i love it!\n",
        "    rfr_ml_modelname = path2folder+\"RFR\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".pkl\"\n",
        "    gpr_ml_modelname = path2folder+\"GPR\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".pkl\"\n",
        "    ann_ml_modelname = path2folder+\"ANN\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".h5\"\n",
        "    mtn_ml_modelname = path2folder+\"MTN\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".h5\"\n",
        "\n",
        "    #COMMENT HERE IF YOU DO NOT WANT TO SAVE THE MODELS\n",
        "    #saving the models\n",
        "    #sklearn are pickles while keras are h5 files\n",
        "    #not saving random forest for now since i am pretty sure it wont be good but it is a big file...\n",
        "    #with open(rfr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(rfr_ml, file)\n",
        "\n",
        "    #with open(gpr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(gpr_ml, file)\n",
        "\n",
        "    #ann_ml.save(ann_ml_modelname)\n",
        "    #model.save(mtn_ml_modelname)\n",
        "      \n",
        "\n",
        "\n",
        "    #the next section stores all the results\n",
        "    for i in range(n_traits):\n",
        "\n",
        "      #this stores the training accuraccy metrics\n",
        "      #print(i)\n",
        "      ann_temp_list = calc_metrics(\"ANN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_ann[:,i],ann_ml_modelname)\n",
        "      rfr_temp_list = calc_metrics(\"RFR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_rfr[:,i],rfr_ml_modelname)\n",
        "      gpr_temp_list = calc_metrics(\"GPR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_gpr[:,i],gpr_ml_modelname)\n",
        "      mtn_temp_list = calc_metrics(\"MTN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_mtn[:,i],mtn_ml_modelname)\n",
        "\n",
        "      #This stores the metrics against the out of bag list\n",
        "      ann_temp_list_valid = calc_metrics(\"ANN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_ann_valid[:,i],ann_ml_modelname)\n",
        "      rfr_temp_list_valid = calc_metrics(\"RFR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_rfr_valid[:,i],rfr_ml_modelname)\n",
        "      gpr_temp_list_valid = calc_metrics(\"GPR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_gpr_valid[:,i],gpr_ml_modelname)\n",
        "      mtn_temp_list_valid = calc_metrics(\"MTN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_mtn_valid[:,i],mtn_ml_modelname)\n",
        "\n",
        "      #appending to the dataframe (training error)\n",
        "      df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(mtn_temp_list,ignore_index=True)\n",
        "\n",
        "      #appending to the dataframe (out of bag error)\n",
        "      df_metrics_valid = df_metrics_valid.append(ann_temp_list_valid,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(rfr_temp_list_valid,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(gpr_temp_list_valid,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(mtn_temp_list_valid,ignore_index=True)\n",
        "    \n",
        "\n",
        "    k=k+1\n",
        "\n",
        "    #saves a temp version of the files\n",
        "    #saving the file to an output\n",
        "  \n",
        "  print(\"Of which\",len(X_train),\"were used for training and\",len(X_test),\"for validation\")\n",
        "  df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/Temp_KFold_TrainingError_csv2.csv\",sep=\";\",decimal=\",\")\n",
        "  df_metrics_valid.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/Temp_KFold_OutOfBag_csv2.csv\",sep=\";\",decimal=\",\")\n",
        "\n",
        "  df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/Temp_KFold_TrainingError_csv1.csv\",sep=\",\",decimal=\".\")\n",
        "  df_metrics_valid.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/Temp_KFold_OutOfBag_csv1.csv\",sep=\",\",decimal=\".\")\n",
        "\n",
        "\n",
        "#saving the file to an output\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_TrainingError_csv2.csv\",sep=\";\",decimal=\",\")\n",
        "df_metrics_valid.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_OutOfBag_csv2.csv\",sep=\";\",decimal=\",\")\n",
        "\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_TrainingError_csv1.csv\",sep=\",\",decimal=\".\")\n",
        "df_metrics_valid.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_OutOfBag_csv1.csv\",sep=\",\",decimal=\".\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop for the following set of LHS samples: [ 200  400  600  800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800\n",
            " 3000]\n",
            "Generating trait table for 200 samples\n",
            "Of which, 10% were taken as out-of-bag: 20\n",
            "Of which 144 were used for training and 36 for validation\n",
            "Generating trait table for 400 samples\n",
            "Of which, 10% were taken as out-of-bag: 40\n",
            "Of which 288 were used for training and 72 for validation\n",
            "Generating trait table for 600 samples\n",
            "Of which, 10% were taken as out-of-bag: 60\n",
            "Of which 432 were used for training and 108 for validation\n",
            "Generating trait table for 800 samples\n",
            "Of which, 10% were taken as out-of-bag: 80\n",
            "Of which 576 were used for training and 144 for validation\n",
            "Generating trait table for 1000 samples\n",
            "Of which, 10% were taken as out-of-bag: 100\n",
            "Of which 720 were used for training and 180 for validation\n",
            "Generating trait table for 1200 samples\n",
            "Of which, 10% were taken as out-of-bag: 120\n",
            "Of which 864 were used for training and 216 for validation\n",
            "Generating trait table for 1400 samples\n",
            "Of which, 10% were taken as out-of-bag: 140\n",
            "Of which 1008 were used for training and 252 for validation\n",
            "Generating trait table for 1600 samples\n",
            "Of which, 10% were taken as out-of-bag: 160\n",
            "Of which 1152 were used for training and 288 for validation\n",
            "Generating trait table for 1800 samples\n",
            "Of which, 10% were taken as out-of-bag: 180\n",
            "Of which 1296 were used for training and 324 for validation\n",
            "Generating trait table for 2000 samples\n",
            "Of which, 10% were taken as out-of-bag: 200\n",
            "Of which 1440 were used for training and 360 for validation\n",
            "Generating trait table for 2200 samples\n",
            "Of which, 10% were taken as out-of-bag: 220\n",
            "Of which 1584 were used for training and 396 for validation\n",
            "Generating trait table for 2400 samples\n",
            "Of which, 10% were taken as out-of-bag: 240\n",
            "Of which 1728 were used for training and 432 for validation\n",
            "Generating trait table for 2600 samples\n",
            "Of which, 10% were taken as out-of-bag: 260\n",
            "Of which 1872 were used for training and 468 for validation\n",
            "Generating trait table for 2800 samples\n",
            "Of which, 10% were taken as out-of-bag: 280\n",
            "Of which 2016 were used for training and 504 for validation\n",
            "Generating trait table for 3000 samples\n",
            "Of which, 10% were taken as out-of-bag: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_u3CWz6hxgt",
        "colab_type": "text"
      },
      "source": [
        "# Plotting stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ubikqsuuuZ",
        "colab_type": "text"
      },
      "source": [
        "For now in R "
      ]
    }
  ]
}