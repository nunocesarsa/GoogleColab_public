{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSRP Lunch - KFolding ML Simulated inversion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGyjR7/c5i6PZMvB4C4nrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunocesarsa/GoogleColab_public/blob/master/DSRP_Lunch_KFolding_ML_Simulated_inversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aYzvp2eS-am",
        "colab_type": "code",
        "outputId": "227f2099-1edb-4ad6-f865-d574e54b9151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#loading up my google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Perhaps this step can be skipped by saving directly to the workspace\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAw7ZP-dRZVS",
        "colab_type": "text"
      },
      "source": [
        "Loading up all the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfQQg-gsPHFO",
        "colab_type": "code",
        "outputId": "02d45943-c601-48bb-a32a-0c188d35f07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#package instalation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "!pip install pysptools\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prosail in /usr/local/lib/python3.6/dist-packages (2.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.17.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.47.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (45.1.0)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Requirement already satisfied: lhsmdu in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Requirement already satisfied: pysptools in /usr/local/lib/python3.6/dist-packages (0.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKBHNWSSXLyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#package to for operations on spectral data\n",
        "import pysptools as sptool \n",
        "from pysptools import distance\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "#machine learning stuff\n",
        "#NEURAL NETWORK - Keras will be updated soon so this colab will also have to be changed\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor as ANN_reg #this is a simpler neural network package\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#ignore the warning for now\n",
        "\n",
        "#Random FOREST\n",
        "# Fitting Random Forest Regression to the dataset \n",
        "# import the regressor \n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#Gaussian processes\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
        "\n",
        "#aux functions\n",
        "from sklearn.preprocessing import MinMaxScaler #this is to standardize the input data [not used for now]\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9JlpBhwXU-Q",
        "colab_type": "text"
      },
      "source": [
        "# Generating a trait space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9TXXAtnXPfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of samples\n",
        "train_n3000 = 3000\n",
        "train_n1500 = 1500\n",
        "train_n0500 = 500\n",
        "\n",
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "LHS_train3000 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n3000 ) #the package has a more advanced method but it is too slow to process\n",
        "LHS_train1500 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n1500 )\n",
        "LHS_train0500 = lhsmdu.createRandomStandardUniformMatrix(n_traits,train_n0500 )\n",
        "\n",
        "#max_n=1 #this value should go from 1 to 2, so i make it change from 0 to 1 here and then add 1 later\n",
        "max_cab=79. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.005\n",
        "min_cm = 0.005\n",
        "min_lai = .5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ZhdQ5sYLv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing function inputs\n",
        "pd_traits0500 = pd.DataFrame.transpose(pd.DataFrame(LHS_train0500))\n",
        "pd_traits1500 = pd.DataFrame.transpose(pd.DataFrame(LHS_train1500))\n",
        "pd_traits3000 = pd.DataFrame.transpose(pd.DataFrame(LHS_train3000))\n",
        "\n",
        "pd_traits0500.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "pd_traits1500.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "pd_traits3000.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "#pd_traits0500[\"car\"]=pd_traits0500[\"car\"]*max_car+1 \n",
        "pd_traits0500[\"cab\"]=pd_traits0500[\"cab\"]*max_cab+min_cab\n",
        "pd_traits0500[\"cw\"] =pd_traits0500[\"cw\"] *max_cw+min_cw\n",
        "pd_traits0500[\"cm\"] =pd_traits0500[\"cm\"] *max_cm+min_cw\n",
        "pd_traits0500[\"lai\"]=pd_traits0500[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "pd_traits1500[\"cab\"]=pd_traits1500[\"cab\"]*max_cab+min_cab\n",
        "pd_traits1500[\"cw\"] =pd_traits1500[\"cw\"] *max_cw+min_cw\n",
        "pd_traits1500[\"cm\"] =pd_traits1500[\"cm\"] *max_cm+min_cm\n",
        "pd_traits1500[\"lai\"]=pd_traits1500[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "pd_traits3000[\"cab\"]=pd_traits3000[\"cab\"]*max_cab+min_cab\n",
        "pd_traits3000[\"cw\"] =pd_traits3000[\"cw\"] *max_cw+min_cw\n",
        "pd_traits3000[\"cm\"] =pd_traits3000[\"cm\"] *max_cm+min_cm\n",
        "pd_traits3000[\"lai\"]=pd_traits3000[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "#lets also create a numpy object for the tratis\n",
        "np_traits0500 = pd_traits0500.iloc[:,:].values\n",
        "np_traits1500 = pd_traits1500.iloc[:,:].values\n",
        "np_traits3000 = pd_traits3000.iloc[:,:].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6rYzm1XYdfT",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the trait space, we need to generate the spectral space. For that we need 3 custom functions:\n",
        "\n",
        "\n",
        "\n",
        "1.   to call prosail given a pre-set group of values of trait\n",
        "2.   to convert hyperspectral to S2 resolution using spectral convolution\n",
        "3.   to iterate through the entire trait space while generating the spectra\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tArZkvpYYw17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  #tts=sol_zen #solar zenith angle\n",
        "  #tto=inc_zen #sensor zenith angle\n",
        "  #psi=raa\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9wEojgY-VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2FIDZFMZEAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCwnGHQjZPez",
        "colab_type": "text"
      },
      "source": [
        "This generates the 3 different sample sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvUjTYMpYxGf",
        "colab_type": "code",
        "outputId": "82bf1cef-92c4-4c85-b710-5e8858a045bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#pd_train_traits[\"n\"]=pd_t\n",
        "np_spectra0500 = Gen_spectra_data(pd_traits0500)\n",
        "np_spectra1500 = Gen_spectra_data(pd_traits1500)\n",
        "np_spectra3000 = Gen_spectra_data(pd_traits3000)\n",
        "\n",
        "print(np_spectra0500.shape)\n",
        "print(np_spectra1500.shape)\n",
        "print(np_spectra3000.shape)\n",
        "\n",
        "#selects only the equivalent 20m bands\n",
        "train_df_0500 = np_spectra0500[:,[1,2,3,4,5,6,8,11,12]]\n",
        "train_df_1500 = np_spectra1500[:,[1,2,3,4,5,6,8,11,12]]\n",
        "train_df_3000 = np_spectra3000[:,[1,2,3,4,5,6,8,11,12]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 13)\n",
            "(1500, 13)\n",
            "(3000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_u3CWz6hxgt",
        "colab_type": "text"
      },
      "source": [
        "# Setting up the models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0USEUladhTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Random forest\n",
        "#initializing the random forest\n",
        "rfr_ml = RandomForestRegressor(n_estimators=1000,random_state=0,\n",
        "                              min_samples_leaf=5,min_samples_split=5,verbose=1)\n",
        "\n",
        "#single task Artificial neural network\n",
        "ann_ml = Sequential()\n",
        "#ann_ml.add(Dense(9, input_dim=9, activation='linear'))\n",
        "ann_ml.add(Dense(10, input_dim=9, activation='tanh'))\n",
        "ann_ml.add(Dense(6, activation='relu'))\n",
        "ann_ml.add(Dense(4)) #indeed this ha to be added in this case without any activ function, the R script added this on its own\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the keras model\n",
        "ann_ml.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#gaussian processes with default acquisition function\n",
        "#initializing the gaussian process\n",
        "gpr_ml = GaussianProcessRegressor(n_restarts_optimizer=50,\n",
        "                                        normalize_y=True,\n",
        "                                        random_state=0)\n",
        "\n",
        "#multi-task neural network - 32 by 16 by 8 per task\n",
        "inputs = Input(shape=(9,))\n",
        "sub1 = Dense(32, activation='tanh')(inputs)\n",
        "sub2 = Dense(16, activation='tanh')(sub1)\n",
        "cab1 = Dense(8, activation='sigmoid')(sub2)\n",
        "cw1  = Dense(8, activation='sigmoid')(sub2)\n",
        "cm1  = Dense(8, activation='sigmoid')(sub2)\n",
        "lai1 = Dense(8, activation='sigmoid')(sub2)\n",
        "cab2 = Dense(1, activation='linear')(cab1)\n",
        "cw2  = Dense(1, activation='linear')(cw1)\n",
        "cm2  = Dense(1, activation='linear')(cm1)\n",
        "lai2 = Dense(1, activation='linear')(lai1)\n",
        "\n",
        "\n",
        "#building the model using keras api\n",
        "model = Model(inputs=inputs, outputs=[cab2,cw2,cm2,lai2])\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ABnaHjFl0-y",
        "colab_type": "text"
      },
      "source": [
        "Before going into the loop, we set up a place for storing the output statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7o87fPQl-ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_names=[\"Model\",\n",
        "              \"NSamples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#this command is enough to set u the k-fold\n",
        "kf = KFold(n_splits=5,shuffle=True,random_state=0) # Define the split "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kNFQTlBmhnw",
        "colab_type": "text"
      },
      "source": [
        "Running for 500 training samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QdT36WMmVHv",
        "colab_type": "code",
        "outputId": "94c4a25d-9a50-42eb-fb01-f0ca2544135f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "\n",
        "#this is a function to re-scale inputs for the neural networks\n",
        "scaler_0500 = MinMaxScaler()\n",
        "\n",
        "#with 500 samples\n",
        "k=1\n",
        "for train_index, test_index in kf.split(train_df_0500):\n",
        "\n",
        "  #subsetting for ith k-fold\n",
        "  X_train, X_test = train_df_0500[train_index], train_df_0500[test_index]\n",
        "  Y_train, Y_test = np_traits0500[train_index], np_traits0500[test_index]\n",
        "  label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  #RF - Training \n",
        "  rfr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #ANN - Training \n",
        "  scaler_0500.fit(Y_train)\n",
        "  Y_train_norm = scaler_0500.transform(Y_train)\n",
        "  ann_ml.fit(X_train,Y_train_norm,epochs=500,verbose=0)\n",
        "\n",
        "  #GPR - Training \n",
        "  gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #Multi task ANN training\n",
        "  cab_train = Y_train_norm[:,0]\n",
        "  cw_train = Y_train_norm[:,1]\n",
        "  cm_train = Y_train_norm[:,2]\n",
        "  lai_train = Y_train_norm[:,3]\n",
        "  \n",
        "  model.fit(X_train,[cab_train,cw_train,cm_train,lai_train],epochs=100,verbose=0)#hides spam on the output\n",
        "\n",
        "  #Prediction\n",
        "  y_ann_0500 = scaler_0500.inverse_transform(ann_ml.predict(X_test))\n",
        "  y_rfr_0500 = rfr_ml.predict(X_test)\n",
        "  y_gpr_0500 = gpr_ml.predict(X_test)\n",
        "\n",
        "  #the multi task network requires a bit more effort\n",
        "  Y_mtn_0500 = model.predict(X_test)\n",
        "\n",
        "\n",
        "  #then we create a stack\n",
        "  cab_pred = Y_mtn_0500[0]\n",
        "  cw_pred  = Y_mtn_0500[1]\n",
        "  cm_pred  = Y_mtn_0500[2]\n",
        "  lai_pred = Y_mtn_0500[3]\n",
        "  np_Y_ann_pred = np.array(cab_pred)\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,cw_pred))\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,cm_pred))\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,lai_pred))\n",
        "\n",
        "  #then we invert the transformation and store to a numpy\n",
        "  y_mtn_0500 = scaler_0500.inverse_transform(np_Y_ann_pred)\n",
        "\n",
        "\n",
        "  for i in range(n_traits):\n",
        "\n",
        "    #creating the temp list\n",
        "    ann_temp_list = {\"Model\":\"ANN\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_ann_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_ann_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_ann_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_ann_0500[:,i])}\n",
        "\n",
        "    rfr_temp_list = {\"Model\":\"RFr\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_rfr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_rfr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_rfr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_rfr_0500[:,i])}\n",
        "\n",
        "    gpr_temp_list = {\"Model\":\"GPR\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "\n",
        "    mtn_temp_list = {\"Model\":\"mtn\",\n",
        "                     \"NSamples\":500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_mtn_0500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_mtn_0500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_mtn_0500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_mtn_0500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_mtn_0500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_mtn_0500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_mtn_0500[:,i])}\n",
        "\n",
        "    #appending to the dataframe\n",
        "    df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(mtn_temp_list,ignore_index=True)\n",
        "\n",
        "  k = k+1\n",
        "\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_0500_4ML.csv\",sep=\";\",decimal=\",\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16_PQ4cXq7hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSQRbw13ruMt",
        "colab_type": "text"
      },
      "source": [
        "For 1500 samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5B8SPParkou",
        "colab_type": "code",
        "outputId": "ba8e743e-c8f2-47a6-b4c3-8002f5d2c4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#the ANN requires that we transform the variables\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "scaler_1500 = MinMaxScaler()\n",
        "\n",
        "#with 1500 samples\n",
        "k=1\n",
        "for train_index, test_index in kf.split(train_df_1500):\n",
        "\n",
        "  print(\"Now in fold nr:\", k)\n",
        "\n",
        "  #subsetting for ith k-fold\n",
        "  X_train, X_test = train_df_1500[train_index], train_df_1500[test_index]\n",
        "  Y_train, Y_test = np_traits1500[train_index], np_traits1500[test_index]\n",
        "  label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  #ANN - Training \n",
        "  scaler_1500.fit(Y_train)\n",
        "  Y_train_norm = scaler_1500.transform(Y_train)\n",
        "  ann_ml.fit(X_train,Y_train_norm,epochs=1500,verbose=0)\n",
        "\n",
        "  #RF - Training n\n",
        "  rfr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #GPR - Training \n",
        "  gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #Multi task ANN training\n",
        "  cab_train = Y_train_norm[:,0]\n",
        "  cw_train = Y_train_norm[:,1]\n",
        "  cm_train = Y_train_norm[:,2]\n",
        "  lai_train = Y_train_norm[:,3]\n",
        "  \n",
        "  model.fit(X_train,[cab_train,cw_train,cm_train,lai_train],epochs=100,verbose=0)#hides spam on the output\n",
        "\n",
        "  #Prediction\n",
        "  y_ann_1500 = scaler_1500.inverse_transform(ann_ml.predict(X_test))\n",
        "  y_rfr_1500 = rfr_ml.predict(X_test)\n",
        "  y_gpr_1500 = gpr_ml.predict(X_test)\n",
        "\n",
        "  #the multi task network requires a bit more effort\n",
        "  Y_mtn_1500 = model.predict(X_test)\n",
        "\n",
        "  #then we create a stack\n",
        "  cab_pred = Y_mtn_1500[0]\n",
        "  cw_pred  = Y_mtn_1500[1]\n",
        "  cm_pred  = Y_mtn_1500[2]\n",
        "  lai_pred = Y_mtn_1500[3]\n",
        "  np_Y_ann_pred = np.array(cab_pred)\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,cw_pred))\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,cm_pred))\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,lai_pred))\n",
        "\n",
        "  #then we invert the transformation and store to a numpy\n",
        "  y_mtn_1500 = scaler_1500.inverse_transform(np_Y_ann_pred)\n",
        "\n",
        "\n",
        "  for i in range(n_traits):\n",
        "\n",
        "    #creating the temp list\n",
        "    ann_temp_list = {\"Model\":\"ANN\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_ann_1500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_ann_1500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_ann_1500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_ann_1500[:,i])}\n",
        "\n",
        "    rfr_temp_list = {\"Model\":\"RFr\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_rfr_1500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_rfr_1500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_rfr_1500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_rfr_1500[:,i])}\n",
        "\n",
        "    gpr_temp_list = {\"Model\":\"GPR\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_1500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_1500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_1500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_gpr_1500[:,i])}\n",
        "\n",
        "    mtn_temp_list = {\"Model\":\"mtn\",\n",
        "                     \"NSamples\":1500,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_mtn_1500[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_mtn_1500[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_mtn_1500[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_mtn_1500[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_mtn_1500[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_mtn_1500[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_mtn_1500[:,i])}\n",
        "\n",
        "  #appending to the dataframe\n",
        "    df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(mtn_temp_list,ignore_index=True)\n",
        "\n",
        "\n",
        "  k = k+1\n",
        "\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_1500_4ML.csv\",sep=\";\",decimal=\",\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now in fold nr: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now in fold nr: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now in fold nr: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now in fold nr: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now in fold nr: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2v3CqhLs1-w",
        "colab_type": "code",
        "outputId": "48b369f4-2d80-451a-f7e0-4c15401da4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#df_metrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>NSamples</th>\n",
              "      <th>Variable</th>\n",
              "      <th>Fold_nr</th>\n",
              "      <th>ExplVar</th>\n",
              "      <th>Max_err</th>\n",
              "      <th>Mean_abs_Err</th>\n",
              "      <th>Mean_sqr_err</th>\n",
              "      <th>Median_abs_err</th>\n",
              "      <th>r2</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ANN</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.991503</td>\n",
              "      <td>8.288998</td>\n",
              "      <td>1.677632</td>\n",
              "      <td>5.027419e+00</td>\n",
              "      <td>1.212710</td>\n",
              "      <td>0.991363</td>\n",
              "      <td>4.933098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RFr</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.992678</td>\n",
              "      <td>14.155291</td>\n",
              "      <td>1.241351</td>\n",
              "      <td>4.262245e+00</td>\n",
              "      <td>0.892719</td>\n",
              "      <td>0.992678</td>\n",
              "      <td>3.137953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GPR</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.999963</td>\n",
              "      <td>1.067922</td>\n",
              "      <td>0.059297</td>\n",
              "      <td>2.270711e-02</td>\n",
              "      <td>0.024714</td>\n",
              "      <td>0.999961</td>\n",
              "      <td>0.264224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mtn</td>\n",
              "      <td>500</td>\n",
              "      <td>cab</td>\n",
              "      <td>1</td>\n",
              "      <td>0.997711</td>\n",
              "      <td>4.326161</td>\n",
              "      <td>0.815881</td>\n",
              "      <td>1.385892e+00</td>\n",
              "      <td>0.519396</td>\n",
              "      <td>0.997619</td>\n",
              "      <td>2.194434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ANN</td>\n",
              "      <td>500</td>\n",
              "      <td>cw</td>\n",
              "      <td>1</td>\n",
              "      <td>0.970710</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>1.757736e-07</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.969987</td>\n",
              "      <td>3.816482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>mtn</td>\n",
              "      <td>1500</td>\n",
              "      <td>cm</td>\n",
              "      <td>5</td>\n",
              "      <td>0.994836</td>\n",
              "      <td>0.001495</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>2.743395e-08</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.994835</td>\n",
              "      <td>1.266226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>ANN</td>\n",
              "      <td>1500</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.991781</td>\n",
              "      <td>0.976543</td>\n",
              "      <td>0.207938</td>\n",
              "      <td>7.262601e-02</td>\n",
              "      <td>0.162331</td>\n",
              "      <td>0.991775</td>\n",
              "      <td>6.438858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>RFr</td>\n",
              "      <td>1500</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.916048</td>\n",
              "      <td>3.904220</td>\n",
              "      <td>0.608495</td>\n",
              "      <td>7.431306e-01</td>\n",
              "      <td>0.410513</td>\n",
              "      <td>0.915837</td>\n",
              "      <td>21.342844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>GPR</td>\n",
              "      <td>1500</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>0.089208</td>\n",
              "      <td>0.008898</td>\n",
              "      <td>1.540369e-04</td>\n",
              "      <td>0.006586</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>0.373101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>mtn</td>\n",
              "      <td>1500</td>\n",
              "      <td>lai</td>\n",
              "      <td>5</td>\n",
              "      <td>0.990445</td>\n",
              "      <td>1.375150</td>\n",
              "      <td>0.236542</td>\n",
              "      <td>9.748612e-02</td>\n",
              "      <td>0.200035</td>\n",
              "      <td>0.988959</td>\n",
              "      <td>4.616858</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Model NSamples Variable  ... Median_abs_err        r2       MAPE\n",
              "0     ANN      500      cab  ...       1.212710  0.991363   4.933098\n",
              "1     RFr      500      cab  ...       0.892719  0.992678   3.137953\n",
              "2     GPR      500      cab  ...       0.024714  0.999961   0.264224\n",
              "3     mtn      500      cab  ...       0.519396  0.997619   2.194434\n",
              "4     ANN      500       cw  ...       0.000225  0.969987   3.816482\n",
              "..    ...      ...      ...  ...            ...       ...        ...\n",
              "155   mtn     1500       cm  ...       0.000077  0.994835   1.266226\n",
              "156   ANN     1500      lai  ...       0.162331  0.991775   6.438858\n",
              "157   RFr     1500      lai  ...       0.410513  0.915837  21.342844\n",
              "158   GPR     1500      lai  ...       0.006586  0.999983   0.373101\n",
              "159   mtn     1500      lai  ...       0.200035  0.988959   4.616858\n",
              "\n",
              "[160 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9MiY0YAs4zZ",
        "colab_type": "text"
      },
      "source": [
        "For 3000 samples\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vThEq5ens3Iv",
        "colab_type": "code",
        "outputId": "39223833-8978-44fb-86f8-fe9fbbb80c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "scaler_3000 = MinMaxScaler()\n",
        "#with 500 samples\n",
        "k=1\n",
        "\n",
        "for train_index, test_index in kf.split(train_df_3000):\n",
        "\n",
        "  #subsetting for ith k-fold\n",
        "  X_train, X_test = train_df_3000[train_index], train_df_3000[test_index]\n",
        "  Y_train, Y_test = np_traits3000[train_index], np_traits3000[test_index]\n",
        "  label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  #ANN - Training \n",
        "  scaler_3000.fit(Y_train)\n",
        "  Y_train_norm = scaler_3000.transform(Y_train)\n",
        "  ann_ml.fit(X_train,Y_train_norm,epochs=3000,verbose=0)\n",
        "\n",
        "  #RF - Training n\n",
        "  rfr_ml.fit(X_train,Y_train)\n",
        "\n",
        "  #GPR - Training \n",
        "  gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "\n",
        "  #Multi task ANN training\n",
        "  cab_train = Y_train_norm[:,0]\n",
        "  cw_train = Y_train_norm[:,1]\n",
        "  cm_train = Y_train_norm[:,2]\n",
        "  lai_train = Y_train_norm[:,3]\n",
        "  \n",
        "  model.fit(X_train,[cab_train,cw_train,cm_train,lai_train],epochs=100,verbose=0)#hides spam on the output\n",
        "\n",
        "  #Prediction\n",
        "  y_ann_3000 = scaler_0500.inverse_transform(ann_ml.predict(X_test))\n",
        "  y_rfr_3000 = rfr_ml.predict(X_test)\n",
        "  y_gpr_3000 = gpr_ml.predict(X_test)\n",
        "\n",
        "  #the multi task network requires a bit more effort\n",
        "  Y_mtn_3000 = model.predict(X_test)\n",
        "\n",
        "  #then we create a stack\n",
        "  cab_pred = Y_mtn_3000[0]\n",
        "  cw_pred  = Y_mtn_3000[1]\n",
        "  cm_pred  = Y_mtn_3000[2]\n",
        "  lai_pred = Y_mtn_3000[3]\n",
        "  np_Y_ann_pred = np.array(cab_pred)\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,cw_pred))\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,cm_pred))\n",
        "  np_Y_ann_pred = np.hstack((np_Y_ann_pred,lai_pred))\n",
        "\n",
        "  #then we invert the transformation and store to a numpy\n",
        "  y_mtn_3000 = scaler_3000.inverse_transform(np_Y_ann_pred)\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(n_traits):\n",
        "\n",
        "    #creating the temp list\n",
        "    ann_temp_list = {\"Model\":\"ANN\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_ann_3000[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_ann_3000[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_ann_3000[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_ann_3000[:,i])}\n",
        "\n",
        "    rfr_temp_list = {\"Model\":\"RFr\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_rfr_3000[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_rfr_3000[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_rfr_3000[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_rfr_3000[:,i])}\n",
        "\n",
        "    gpr_temp_list = {\"Model\":\"GPR\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_3000[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_3000[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_3000[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_gpr_3000[:,i])}\n",
        "\n",
        "    mtn_temp_list = {\"Model\":\"mtn\",\n",
        "                     \"NSamples\":3000,\n",
        "                     \"Variable\":label_names[i],\n",
        "                     \"Fold_nr\":k,\n",
        "                     \"ExplVar\": metrics.explained_variance_score(Y_test[:,i], y_mtn_3000[:,i]),\n",
        "                     \"Max_err\": metrics.max_error(Y_test[:,i], y_mtn_3000[:,i]),\n",
        "                     \"Mean_abs_Err\": metrics.mean_absolute_error(Y_test[:,i], y_mtn_3000[:,i]),\n",
        "                     \"Mean_sqr_err\": metrics.mean_squared_error(Y_test[:,i], y_mtn_3000[:,i]),\n",
        "                     #\"Mean_sqr_lg_err\": metrics.mean_squared_log_error(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     \"Median_abs_err\" : metrics.median_absolute_error(Y_test[:,i], y_mtn_3000[:,i]),\n",
        "                     \"r2\": metrics.r2_score(Y_test[:,i], y_mtn_3000[:,i]),\n",
        "                     #\"Mean_poiss_dev\" : metrics.mean_poisson_deviance(Y_test[:,i], y_gpr_0500[:,i]),\n",
        "                     #\"Mean_gamma_dev\" : metrics.mean_gamma_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     #\"Mean_tweed_dev\" : metrics.mean_tweedie_deviance(Y_test[:,i], y_gpr_0500[:,i])}\n",
        "                     \"MAPE\": mean_absolute_percentage_error(Y_test[:,i], y_mtn_3000[:,i])}\n",
        "\n",
        "  #appending to the dataframe\n",
        "    df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "    df_metrics = df_metrics.append(mtn_temp_list,ignore_index=True)\n",
        "  k = k+1\n",
        "\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/DSRP_Lunch_outputs/KFold_3000_4ML.csv\",sep=\";\",decimal=\",\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.6s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   11.7s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRsM4m2tuxxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ubikqsuuuZ",
        "colab_type": "text"
      },
      "source": [
        "The exploratory plots are made in R"
      ]
    }
  ]
}